apiVersion: serving.kubeflow.org/v1beta1
kind: InferenceService
metadata:
  name: inference-{{ workflow.parameters.run_name }}
  annotations:
    autoscaling.knative.dev/scaleToZeroPodRetentionPeriod: 20m
spec:
  predictor:
    minReplicas: 0
    maxReplicas: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: gpu.nvidia.com/class
                  operator: In
                  values:
                    - "{{workflow.parameters.inference_gpu}}"
                - key: topology.kubernetes.io/region
                  operator: In
                  values:
                    - "{{workflow.parameters.region}}"
    containers:
      - name: kfserving-container
        image: "{{workflow.parameters.inference_image}}:{{workflow.parameters.inference_tag}}"
        imagePullPolicy: IfNotPresent
        command: {{inputs.parameters.command}}
        env:
          - name: STORAGE_URI
            value: pvc://{{ workflow.parameters.pvc }}/models/
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: 4
            memory: 8Gi
          limits:
            nvidia.com/gpu: 1
            cpu: 12
            memory: 60Gi
